# ğŸ§  CNN Deep Learning Project (MNIST)

## ğŸ“Œ Project Overview
This project explores Convolutional Neural Networks (CNNs) for image classification using the MNIST handwritten digits dataset.  
The goal is to analyze how architectural changes affect **performance, accuracy, training time, and model cost**.

A **baseline CNN** is implemented first, followed by two different **architecture variations** to observe trade-offs between model complexity and performance.

---

## ğŸ“Š Dataset
- **Dataset:** MNIST
- **Classes:** 10 (digits 0â€“9)
- **Input Shape:** 28 Ã— 28 grayscale images
- **Preprocessing:**
  - Normalization (pixel values scaled to [0,1])
  - Reshaping to `(28, 28, 1)`
  - One-hot encoding for labels

---

## ğŸ—ï¸ Model Architectures

### 1ï¸âƒ£ CNN Basic (Reference Model)
- Conv2D (32 filters) â†’ MaxPooling
- Conv2D (64 filters) â†’ MaxPooling
- Flatten
- Dense (128)
- Output Dense (10, Softmax)

This model serves as the **baseline** for performance and cost comparison.

---

### 2ï¸âƒ£ CNN Variation 1 â€“ Lightweight Model
- Reduced number of filters (16 â†’ 32)
- Same overall structure as the baseline
- Significantly fewer parameters

**Goal:** Reduce computational cost while maintaining accuracy.

---

### 3ï¸âƒ£ CNN Variation 2 â€“ Deeper Model
- Additional convolutional layer added
- Increased representational capacity

**Goal:** Observe accuracy gains versus increased training time and parameter count.

---

## âš™ï¸ Training Setup
- **Optimizer:** Adam
- **Loss Function:** Categorical Crossentropy
- **Metric:** Accuracy
- **Epochs:** 10
- **Batch Size:** 32
- **Validation Split:** 20%

---

## ğŸ“ˆ Evaluation Metrics
Each model is evaluated using:
- Test Accuracy
- Test Loss
- Training Time (seconds)
- Total Trainable Parameters

These metrics allow direct comparison of **performance vs. cost**.

---

## ğŸ” Key Observations
- The **baseline CNN** achieves high accuracy with moderate cost.
- The **lightweight model** significantly reduces parameters and training time with minimal accuracy loss.
- The **deeper model** slightly improves accuracy but increases training time and complexity.
- Dense layers contribute the majority of model parameters and computational cost.

---

## ğŸ§ª Technologies Used
- Python
- TensorFlow / Keras
- NumPy
- Google Colab

---



## ğŸ¯ Conclusion
This project demonstrates how CNN architecture design directly impacts performance and computational efficiency.  
It provides a clear comparison between **baseline, lightweight, and deeper CNN models**, highlighting practical trade-offs in deep learning model design.

